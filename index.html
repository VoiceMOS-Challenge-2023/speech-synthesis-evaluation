<html>
<head>
<title>Speech Synthesis Evalaution Reading List</title>
</head>
</title>

<body>

<h2>Speech Synthesis Evaluation</h2>

<p>
We have written a review paper on subjective and evaluation of speech synthesis, titled <a href="https://doi.org/10.1250/ast.e24.12">"A review on subjective and objective evaluation of synthetic speech,"</a> for the journal <i>Acoustical Science and Technology,</i> published by the Acoustical Society of Japan.  
This webpage contains the papers cited in our review with links where possible.  We will also add other interesting and relevant papers as we find them.
</p>

<b>Table of Contents</b>
<ul>
  <li>2. Listening tests</li>
  <ul>
    <li>2.1  1980s to early 1990s: intelligibility and comprehension</li>
    <li>2.2  Mid-1990s and 2000s: naturalness, intelligibility, and efforts to standardize</li>
    <li>2.3  2010s to the present: crowdsourcing, MOS, and critiques</li>
  </ul>
  <li>3. Automatic evaluation for synthetic speech</li>
  <ul>
    <li>3.1  Difficulties in automatic evaluation of synthetic speech</li>
    <li>3.2  Speech quality assessment metrics from telephony</li>
    <li>3.3  Models for evaluation of synthetic speech</li>
    <ul>
      <li>3.3.1  Early attempts at machine learning based synthetic speech quality prediction</li>
      <li>3.3.2  Neural network-based synthetic speech quality prediction</li>
      <li>3.3.3  Listener modeling in synthetic speech evaluation</li>
      <li>3.3.4  SSL-based approaches</li>
      <li>3.3.5  Unsupervised approaches for synthetic speech quality prediction</li>
      <li>3.3.6  Beyond predicting quality of synthesized speech</li>
      <li>3.3.7  Predicting rank order and pairwise preferences</li>
      <li>3.3.8  Learning from speech quality prediction in other domains</li>
    </ul>
  </ul>
    <li>4. The VoiceMOS Challenge 2022</li>
    <ul>
      <li>4.1  Data and tracks</li>
      <li>4.2  Baselines</li>
      <li>4.3  Team approaches</li>
    </ul>
    <li>5. The VoiceMOS Challenge 2023</li>
    <ul>
      <li>5.1  Data and tracks</li>
      <li>5.2  Baselines</li>
      <li>5.3  Team approaches</li>
    </ul>
    <li>6.  Future prospects and challenges</li>
    
</ul>



  <h3>2. Listening tests</h3>
    <h4>2.1  1980s to early 1990s: intelligibility and
      comprehension</h4>

<p>[1] R. Van Bezooijen and
  L. C. Pols, <a href="https://doi.org/10.1016/0167-6393(90)90002-Q">"Evaluating text-to-
speech systems: Some methodological aspects,"</a> <i>Speech
Communication,</i> vol. 9, no. 4, pp. 263-270, 1990.</p>

<p>[2] A. S. House, C. Williams, M. H. Hecker, and
  K. D. Kryter, <a href="https://doi.org/10.1121/1.2142744">"Psychoacoustic
    speech tests: A modified rhyme test,"</a> <i>The Journal of the Acoustical Society of America,</i> vol. 35, pp. 1899-1899, 1963.</p>

<p>[3] W. D. Voiers, <a href="">"Evaluating processed speech using the
diagnostic rhyme test,"</a> <i>Speech Technol.,</i> vol. 1, pp. 30-
  39, 1983.</p>

<p>[4] M. Spiegel, M. J. Altom, M. Macchi, and K. Wallace, <a href="https://www.isca-archive.org/sioa_1989/spiegel89_sioa.html">"A monosyllabic test corpus to evaluate the
intelligibility of synthesized and natural speech,"</a> in
<i>Proc. Speech Input/Output Assessment and Speech
Databases,</i> 1989, pp. Vol.2, 5-10. </p> 

<p>[5] U. Jekosch, <a href="https://www.isca-archive.org/sioa_1989/jekosch89_sioa.html">"The cluster-based rhyme test: a segmental synthesis test for open vocabulary,"</a> in <i>Proc. Speech
Input/Output Assessment and Speech Databases,</i> 1989,
pp. Vol.2, 15-18.</p>

<p>[6] J. P. van Santen, <a href="https://www.sciencedirect.com/science/article/pii/S0885230883710041">"Perceptual experiments for diagnostic testing of text-to-speech systems,"</a> <i>Computer
Speech & Language,</i> vol. 7, no. 1, pp. 49-100, 1993.</p>

<p>[7] M. Grice, <a href="https://www.isca-archive.org/sioa_1989/grice89_sioa.html">"Syntactic structures and lexicon requirements for semantically unpredictable sentences in a
number of languages,"</a> in <i>Proc. Speech Input/Output
Assessment and Speech Databases,</i> 1989, pp. Vol.2, 19-
22.</p>

<p>[8] D. Pisoni and S. Hunnicutt, <a href="https://ieeexplore.ieee.org/abstract/document/1170888">"Perceptual evaluation
of MITalk: The MIT unrestricted text-to-speech system,"</a> in <i>ICASSP '80. IEEE International Conference
on Acoustics, Speech, and Signal Processing,</i> vol. 5,
1980, pp. 572-575.</p>

<p>[9] <a href="https://www.itu.int/rec/T-REC-P.800-199608-I">"Methods for subjective determination of transmission
quality,"</a> in <i>ITU-T Rec. P.800.</i> International Telecommunication Union (ITU-R)., 1996.</p>

<p>[10] M. Goldstein, B. Lindstr&#246;m, and O. Till, <a href="https://www.isca-archive.org/icslp_1992/goldstein92b_icslp.html">"Some aspects on
  context and response range effects when assessing naturalness of Swedish sentences generated
by 4 synthesiser systems,"</a> in <i>Proc. 2nd International
Conference on Spoken Language Processing (ICSLP
1992),</i> 1992, pp. 1339-1342.</p>

<p>[11] M. Goldstein, <a href="https://doi.org/10.1016/0167-6393(94)00047-E">"Classification of methods used for
assessment of text-to-speech systems according to
the demands placed on the listener,"</a> <i>Speech Communication,</i> vol. 16, no. 3, pp. 225-244, 1995.</p>


    
    <h4>2.2  Mid-1990s and 2000s: naturalness, intelligibility, and efforts to standardize</h4>

<p>[12] <a href="https://www.itu.int/rec/T-REC-P.85-199406-I/en">"A method for subjective performance assessment of
the quality of speech voice output devices,"</a> in <i>ITU-T
Rec. P.85.</i> International Telecommunication Union
(ITU-R)., 1994.</p>

<p>[13] <a href="https://www.itu.int/rec/T-REC-P.80-199303-S">"Methods for subjective determination of transmission
quality,"</a> in <i>ITU-T Rec. P.80.</a> International Telecommunication Union (ITU-R)., 1993.</p>

<p>[14] C. Beno&#238;t, M. Grice, and V. Hazan, <a href="https://doi.org/10.1016/0167-6393(96)00026-X">"The SUS
test: A method for the assessment of text-to-
speech synthesis intelligibility using Semantically
Unpredictable Sentences,"</a> <i>Speech Communication,</i>
vol. 18, no. 4, pp. 381-392, 1996</p>

<p>[15] S. Itahashi, <a href="http://www.lrec-conf.org/proceedings/lrec2000/html/summary/77.htm">"Guidelines for Japanese speech synthesizer evaluation,"</a> in <i>Proceedings of the Second
International Conference on Language Resources and
Evaluation (LREC’00),</i> M. Gavrilidou, G. Carayannis,
S. Markantonatou, S. Piperidis, and G. Stainhauer,
Eds. Athens, Greece: European Language Resources
Association (ELRA), May 2000. </p>

<p>[16] Y. V. Alvarez and
  M. Huckvale, <a href="https://www.isca-archive.org/icslp_2002/alvarez02_icslp.html">"The
    reliability of the ITU-T P.85 standard for the evaluation of
  text-to-speech systems,"</a> in <i>Proc. 7th International Conference
on Spoken Language Processing (ICSLP 2002),</i> 2002,
pp. 329-332.</p>

<p>[17] D. Sityaev, K. Knill, and T. Burrows, <a href="https://www.isca-archive.org/interspeech_2006/sityaev06_interspeech.html">"Comparison
of the ITU-T P.85 standard to other methods for the
evaluation of text-to-speech systems,"</a> in <i>Proc. Interspeech 2006,</i> 2006.</p>

<p>[18]  L. C. W. Pols and U. Jekosch, <a href="https://doi.org/10.1007/978-1-4612-1894-4_41"><i>A Structured
Way of Looking at the Performance of Text-to-Speech Systems.</i></a> New York, NY: Springer New
York, 1997, pp. 519-527</p>

<p>[19] A. W. Black and K. Tokuda, <a href="https://www.isca-archive.org/interspeech_2005/black05_interspeech.html">"The Blizzard Challenge - 2005: evaluating corpus-based speech synthesis on
common datasets,"</a> in <i>Proc. Interspeech 2005,</i> 2005,
pp. 77-80.</p>

<p>[20] N. Campbell, <a href="https://doi.org/10.1007/978-1-4020-5817-2_2"><i>Evaluation of Speech Synthesis.</i></i> Dordrecht: Springer Netherlands, 2007, pp.
29-64</p>

<p>[21] S. Zielinski, F. Rumsey, and S. Bech, <a href="http://www.aes.org/e-lib/browse.cfm?elib=14393">"On some biases
encountered in modern audio quality listening tests - a review,"</i> <i>Journal of the Audio Engineering Society,</i>
vol. 56, no. 6, pp. 427-451, 2008.</p>

    <h4>2.3  2010s to the present: crowdsourcing, MOS, and critiques</h4>

<p>[22] K. Tokuda, H. Zen, and A. W. Black, <a href="https://doi.org/10.1109/WSS.2002.1224415">"An HMM-based
speech synthesis system applied to English,"</a> in <i>IEEE Speech Synthesis Workshop.</i> IEEE Santa Monica,
2002, pp. 227-230.</p>

<p>[23] X. Wang, J. Yamagishi, M. Todisco, H. Delgado,
A. Nautsch, N. Evans, M. Sahidullah, V. Vestman, T. Kinnunen, K. A. Lee, L. Juvela, P. Alku,
Y.-H. Peng, H.-T. Hwang, Y. Tsao, H.-M. Wang,
S. L. Maguer, M. Becker, F. Henderson, R. Clark,
Y. Zhang, Q. Wang, Y. Jia, K. Onuma, K. Mushika,
T. Kaneda, Y. Jiang, L.-J. Liu, Y.-C. Wu, W.-C.
Huang, T. Toda, K. Tanaka, H. Kameoka, I. Steiner,
D. Matrouf, J.-F. Bonastre, A. Govender, S. Ronanki, J.-X. Zhang, and
  Z.-H. Ling, <a href="https://doi.org/10.1016/j.csl.2020.101114">"ASVspoof 2019:
A large-scale public database of synthesized, converted and replayed speech,"</a> <i>Computer Speech &
Language,</i> vol. 64, p. 101114, 2020. </p>

<p>[24]  F. Ribeiro, D. Florêncio, C. Zhang, and M. Seltzer, 
  "CrowdMOS: An approach for crowdsourcing mean opinion score studies," 
  in 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 
  IEEE, 2011, pp. 2416–2419.</p>

<p>[25] S. Buchholz, J. Latorre, and K. Yanagisawa, 
  "Crowdsourced assessment of speech synthesis," 
  in Crowdsourcing for Speech Processing: Applications to Data, Collection, 
  Transcription and Assessment, M. Eskénazi, G.-A. Levow, H. Meng, G. Parent, 
  and D. Suendermann, Eds. Chichester: John Wiley & Sons, Ltd, 2013, ch. 7, pp. 173–214.</p>

<p>[26] M. Wester, C. Valentini-Botinhao, and G. E. Henter, 
  "Are we using enough listeners? No!—An empiricallysupported critique of interspeech 2014 TTS evaluations,"
  in Proc. Interspeech 2015, 2015, pp. 3476–3480.</p>

<p>[27] "Method for the Subjective Assessment of Intermediate Sound Quality (MUSHRA),"
  in Recommendation ITU-R BS.1534-3. International Telecommunication Union (ITU-R)., 2015.</p>

<p>[28] R. C. Streijl, S. Winkler, and D. S. Hands, 
  "Mean opinion score (MOS) revisited: methods and applications, limitations and alternatives,"
  Multimedia Systems, vol. 22, no. 2, pp. 213–227, 2016.</p>

<p>[29] P. Wagner, J. Beskow, S. Betz, J. Edlund, J. Gustafson, G. Eje Henter, S. Le Maguer, Z. Malisz, 
  Éva Székely, C. Tånnander, and J. Voße, "Speech Synthesis Evaluation — State-of-the-Art Assessment 
  and Suggestion for a Novel Research Program," in Proc. 10th ISCA Workshop on Speech Synthesis (SSW 10), 
  2019, pp. 105–110.</p>

<p>[30] M. Wester, O. Watts, and G. E. Henter, 
  "Evaluating comprehension of natural and synthetic conversational speech,"
  in Proc. Speech Prosody 2016, 2016, pp. 766–770.</p>

<p>[31] J. Mendelson and M. P. Aylett, "Beyond the Listening Test: An Interactive Approach to TTS Evaluation,"
  in Proc. Interspeech 2017, 2017, pp. 249–253.</p>

<p>[32] R. Clark, H. Silen, T. Kenter, and R. Leith, "Evaluating Long-form Text-to-Speech: Comparing the Ratings of Sentences and Paragraphs,"
  in Proc. 10th ISCA Workshop on Speech Synthesis (SSW 10), 2019, pp. 99–104.</p>

<p>[33] J. O'Mahony, P. O.Gallegos, C. Lai, and S. King, 
  "Factors affecting the evaluation of synthetic speech in context," 
  in The 11th ISCA Speech Synthesis Workshop (SSW11). International 
  Speech Communication Association, 2021, pp. 148–153.</p>

<p>[34] R. Dall, J. Yamagishi, and S. King, 
  "Rating naturalness in speech synthesis: The effect of style and expectation,"
  in Proc. 7th International Conference on Speech Prosody 2014, 2014, pp. 1012–1016.</p>

<p>[35] S. Shirali-Shahreza and G. Penn, "Better Replacement for TTS Naturalness Evaluation," 
  in Proc. 12th ISCA Speech Synthesis Workshop (SSW2023), 2023, pp.197–203.</p>

<p>[36] S. King, "Measuring a decade of progress in text-to-speech," Loquens, vol. 1, no. 1, p. e006, 2014.</p>

<p>[37] F. Seebauer, M. Kuhlmann, R. Haeb-Umbach, and P. Wagner, 
  "Re-examining the quality dimensions of synthetic speech," 
  in Proc. 12th ISCA Speech Synthesis Workshop (SSW2023), 2023, pp. 34–40.</p>

<p>[38] S. Shirali-Shahreza and G. Penn, 
  "MOS Naturalness and the Quest for Human-Like Speech,"
  in 2018 IEEE Spoken Language Technology Workshop (SLT), 2018, pp. 346–352.</p>

<p>[39] J. Camp, T. Kenter, L. Finkelstein, and R. Clark, 
  "MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors,"
  in Proc. INTERSPEECH 2023, 2023, pp. 1090–1094.</p>

<p>[40]  Y. Yasuda and T. Toda, 
  "Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech Based on Tail Probabilities,"
  in Proc. INTERSPEECH 2023, 2023, pp. 5491–5495.</p>

<p>[41] E. Cooper and J. Yamagishi, "Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech,"
  in Proc. INTERSPEECH 2023, 2023, pp. 1104–1108.</p>

<p>[42] A. Kirkland, S. Mehta, H. Lameris, G. E. Henter, E. Szekely, and J. Gustafson, 
  "Stuck in the MOS pit: A critical analysis of MOS test methodology in TTS evaluation,"
  in Proc. 12th ISCA Speech Synthesis Workshop (SSW2023), 2023, pp. 41–47.</p>

<p>[43] S. Le Maguer, S. King, and N. Harte, 
  "The limits of the mean opinion score for speech synthesis evaluation,"
  Computer Speech & Language, vol. 84, p. 101577, 2024. </p>
    
  <h3>3. Automatic evaluation for synthetic speech</h3>

<p>[44] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, "BLEU: a Method for Automatic Evaluation of Machine Translation,"
  in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311–318.</p>
  
    <h4>3.1  Difficulties in automatic evaluation of synthetic speech</h4>

    <h4>3.2  Speech quality assessment metrics from telephony</h4>

<p>[45] R. Kubichek, "Mel-cepstral distance measure for objective speech quality assessment,"
  in Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing, vol. 1, 1993, pp. 125–128 vol.1.</p>

<p>[46] J. Kominek, T. Schultz, and A. W. Black, 
  "Synthesizer voice quality of new languages calibrated with mean Mel cepstral distortion,"
  in Proc. Speech Technology for Under-Resourced Languages (SLTU-2008), 2008, pp. 63–68.</p>

<p>[47] "Perceptual evaluation of speech quality (PESQ): 
  An objective method for end-to-end speech quality assessment of narrow-band telephone networks 
  and speech codecs," in ITU-T Recommendation P.862, 2001.</p>

<p>[48] S. Ipswich, "PESQ: An Introduction White Paper," 2001.</p>

<p>[49] M. Cernak and M. Rusko, "An evaluation of synthetic speech using the PESQ measure,"
  in Proc. European Congress on Acoustics, 2005, pp. 2725–2728.</p>

<p>[50] F. Hinterleitner, S. Zabel, S. Möller, L. Leutelt, and C. Norrenbrock, 
  "Predicting the quality of synthesized speech using reference-based prediction measures,"
  in Konferenz Elektronische Sprachsignalverarbeitung. TUDpress, Dresden, 2011, pp. 99–106.</p>

<p>[51] L. Latacz and W. Verhelst, "Double-ended prediction of the naturalness ratings of 
  the Blizzard Challenge 2008-2013," in Proc. Interspeech 2015, 2015, pp. 3486–3490.
</p>

<p>[52] "Single-ended method for objective speech quality assessment in narrow-band telephony applications,"
  in ITU-T Rec. P.563, 2004.</p>

<p>[53] L. Malfait, J. Berger, and M. Kastner, 
  "P. 563—The ITU-T standard for single-ended speech quality assessment,"
  IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 6, pp. 1924–1934, 2006.</p>

<p>[54] D.-S. Kim and A. Tarraf, "Anique+: A new american national standard for non-intrusive 
  estimation of narrowband speech quality," Bell Labs Technical Journal, vol. 12, no. 1, pp. 221–236, 
  2007.</p>

<p>[55] T. H. Falk, S. Möller, V. Karaiskos, and S. King, 
  "Improving instrumental quality prediction performance for the Blizzard Challenge,"
  in Proc. Blizzard Challenge Workshop, 2008.</p>

<p>[56] T. H.Falk and S. Moller, "Towards signal-based instrumental quality diagnosis for text-to-speech 
  systems," IEEE Signal Processing Letters, vol. 15, pp. 781–784, 2008.</p>

<p>[57] T. Yoshimura, G. E. Henter, O. Watts, M. Wester, J. Yamagishi, and K. Tokuda, 
  "A Hierarchical Predictor of Synthetic Speech Naturalness Using Neural Networks,"
  in Proc. Interspeech 2016, 2016, pp. 342–346.</p>

<p>[58] R. Clark and K. Dusterhoff, "Objective methods for evaluating synthetic intonation,"
  in Proc. Eurospeech '99, Sixth European Conf. on Speech Communication and Technology, Budapest, 
  Hungary, 1999, pp. 1623—-1626.</p>

<p>[59] U. Remes, R. Karhila, and M. Kurimo, 
  "Objective evaluation measures for speaker-adaptive HMM-TTS systems,"
  in Eighth ISCA Workshop on Speech Synthesis, 2013.</p>

    
    <h4>3.3  Models for evaluation of synthetic speech</h4>

<p>[60] F. Hinterleitner, S. Zander, K.-P. Engelbrecht, and S. Möller, 
  "On the use of automatic speech recognizers for the quality and intelligibility prediction of 
  synthetic speech," in Konferenz Elektronische Sprachsignalverarbeitung. TUDpress, Dresden, 2015, pp. 
  105–111.</p>

<p>[61] O. Sharoni, R. Shenberg, and E. Cooper, 
  "SASPEECH: A Hebrew single speaker dataset for text to speech and voice conversion,"
  in Proc. Interspeech, 2023.</p>

<p>[62] S. Mehta, R. Tu, J. Beskow, É. Székely, and G. E. Henter, 
  "Matcha-TTS: A fast TTS architecture with conditional flow matching,"
  in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing 
  (ICASSP) (to appear), 2024.</p>

<p>[63]  W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang, J. Raiman, and J. Miller, 
  "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,"
  in International Conference on Learning Representations, 2018.
</p>
    
    
      <h5>3.3.1  Early attempts at machine learning based synthetic speech quality prediction</h5>

<p>[64] F. Hinterleitner, S. Möller, T. H. Falk, and T. Polzehl, 
  "Comparison of approaches for instrumentally predicting the quality of text-to-speech systems: 
  Data from Blizzard Challenges 2008 and 2009,"
  in Blizzard Challenge Workshop, vol. 2010, 2010, pp. 48–60.</p>

<p>[65] V. Karaiskos, S. King, R. A. Clark, and C. Mayo, 
  "The Blizzard Challenge 2008,"
  in Proc. Blizzard Challenge Workshop. Citeseer, 2008.</p>

<p>[66] A. W. Black, S. King, and K. Tokuda, "The Blizzard Challenge 2009,"
  in Proc. Blizzard Challenge, 2009, pp. 1–24.</p>

<p>[67] S. King and V. Karaiskos, "The Blizzard Challenge 2011," 2012.</p>

<p>[68] C. R. Norrenbrock, F. Hinterleitner, U. Heute, and S. Möller, 
  "Towards perceptual quality modeling of synthesized audiobooks – Blizzard Challenge 2012,"
  in Blizzard Challenge Workshop, 2012.</p>


      <h5>3.3.2  Neural network-based synthetic speech quality prediction</h5>

<p>[69] T. Toda, L.-H. Chen, D. Saito, F. Villavicencio, M. Wester, Z. Wu, and J. Yamagishi, 
  "The Voice Conversion Challenge 2016," in Proc. Interspeech, 2016, pp. 1632–1636.</p>

<p>[70] J. Lorenzo-Trueba, J. Yamagishi, T. Toda, D. Saito, F. Villavicencio, T. Kinnunen, 
  and Z. Ling, "The Voice Conversion Challenge 2018: Promoting Development of Parallel and 
  Nonparallel Methods," in Proc. Odyssey, 2018, pp. 195–202.</p>

<p>[71]  Y. Zhao, W.-C. Huang, X. Tian, J. Yamagishi, R. K. Das, T. Kinnunen, Z. Ling, and T. Toda, 
  "Voice Conversion Challenge 2020 – Intra-lingual semi-parallel and cross-lingual voice conversion," 
in Proc. Joint Workshop for the BC and VCC 2020, 2020, pp. 80–98.</p>

<p>[72] W.-C.Huang, L. P. Violeta, S. Liu, J. Shi, and T. Toda, 
  "The Singing Voice Conversion Challenge 2023," in Proc. ASRU, 2023.</p>

<p>[73]</p>

<p>[74]</p>

<p>[75]</p>

<p>[76]</p>

<p>[77]</p>

<p>[78]</p>

<p>[79]</p>

      <h5>3.3.3  Listener modeling in synthetic speech evaluation</h5>

<p>[80]</p>

<p>[81]</p>

      <h5>3.3.4  SSL-based approaches</h5>

<p>[82]</p>

<p>[83]</p>

<p>[84]</p>

<p>[85]</p>

<p>[86]</p>

<p>[87]</p>

<p>[88]</p>

<p>[89]</p>

<p>[90]</p>

<p>[91]</p>

<p>[92]</p>

<p>[93]</p>

      <h5>3.3.5  Unsupervised approaches for synthetic speech quality prediction</h5>

<p>[94]</p>

<p>[95]</p>

<p>[96]</p>

<p>[97]</p>

<p>[98]</p>

<p>[99]</p>

      <h5>3.3.6  Beyond predicting quality of synthesized speech</h5>

<p>[100]</p>

<p>[101]</p>

<p>[102]</p>

<p>[103]</p>

<p>[104]</p>

<p>[105]</p>

<p>[106]</p>

<p>[107]</p>

<p>[108]</p>

<p>[109]</p>

<p>[110]</p>

<p>[111]</p>

<p>[112]</p>

<p>[113]</p>

<p>[114]</p>

<p>[115]</p>

<p>[116]</p>

      <h5>3.3.7  Predicting rank order and pairwise preferences</h5>

<p>[117]</p>

<p>[118]</p>

<p>[119]</p>

<p>[120]</p>
      
      <h5>3.3.8  Learning from speech quality prediction in other domains</h5>

<p>[121]</p>

<p>[122]</p>

<p>[123]</p>

<p>[124]</p>

<p>[125]</p>      
      
  <h3>4. The VoiceMOS Challenge 2022</h3>

<p>[126]</p>

    <h4>4.1  Data and tracks</h4>

<p>[127]</p>

<p>[128]</p>

<p>[129]</p>

<p>[130]</p>

<p>[131]</p>

<p>[132]</p>

    <h4>4.2  Baselines</h4>
    
    <h4>4.3  Team approaches</h4>

<p>[133]</p>

<p>[134]</p>

<p>[135]</p>

<p>[136]</p>

<p>[137]</p>

<p>[138]</p>

<p>[139]</p>

<p>[140]</p>

<p>[141]</p>

<p>[142]</p>

<p>[143]</p>
    
  <h3>5. The VoiceMOS Challenge 2023</h3>

<p>[144]</p>

    <h4>5.1  Data and tracks</h4>

<p>[145]</p>

<p>[146]</p>

<p>[147]</p>

    <h4>5.2  Baselines</h4>

    <h4>5.3  Team approaches</h4>

<p>[148]</p>

<p>[149]</p>

<p>[150]</p>
    
  <h3>6.  Future prospects and challenges</h3>

<p>[151]</p>

<p>[152]</p>







