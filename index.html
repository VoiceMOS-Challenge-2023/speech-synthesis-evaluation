<html>
<head>
<title>Speech Synthesis Evalaution Reading List</title>
</head>
</title>

<body>

<h2>Speech Synthesis Evaluation</h2>

<p>
We have written a review paper on subjective and evaluation of speech synthesis, titled <a href="https://doi.org/10.1250/ast.e24.12">"A review on subjective and objective evaluation of synthetic speech,"</a> for the journal <i>Acoustical Science and Technology,</i> published by the Acoustical Society of Japan.  
This webpage contains the papers cited in our review with links where possible.  We will also add other interesting and relevant papers as we find them.
</p>

<b>Table of Contents</b>
<ul>
  <li>2. Listening tests</li>
  <ul>
    <li>2.1  1980s to early 1990s: intelligibility and comprehension</li>
    <li>2.2  Mid-1990s and 2000s: naturalness, intelligibility, and efforts to standardize</li>
    <li>2.3  2010s to the present: crowdsourcing, MOS, and critiques</li>
  </ul>
  <li>3. Automatic evaluation for synthetic speech</li>
  <ul>
    <li>3.1  Difficulties in automatic evaluation of synthetic speech</li>
    <li>3.2  Speech quality assessment metrics from telephony</li>
    <li>3.3  Models for evaluation of synthetic speech</li>
    <ul>
      <li>3.3.1  Early attempts at machine learning based synthetic speech quality prediction</li>
      <li>3.3.2  Neural network-based synthetic speech quality prediction</li>
      <li>3.3.3  Listener modeling in synthetic speech evaluation</li>
      <li>3.3.4  SSL-based approaches</li>
      <li>3.3.5  Unsupervised approaches for synthetic speech quality prediction</li>
      <li>3.3.6  Beyond predicting quality of synthesized speech</li>
      <li>3.3.7  Predicting rank order and pairwise preferences</li>
      <li>3.3.8  Learning from speech quality prediction in other domains</li>
    </ul>
  </ul>
    <li>4. The VoiceMOS Challenge 2022</li>
    <ul>
      <li>4.1  Data and tracks</li>
      <li>4.2  Baselines</li>
      <li>4.3  Team approaches</li>
    </ul>
    <li>5. The VoiceMOS Challenge 2023</li>
    <ul>
      <li>5.1  Data and tracks</li>
      <li>5.2  Baselines</li>
      <li>5.3  Team approaches</li>
    </ul>
    <li>6.  Future prospects and challenges</li>
    
</ul>



  <h3>2. Listening tests</h3>
    <h4>2.1  1980s to early 1990s: intelligibility and
      comprehension</h4>

<p>[1] R. Van Bezooijen and
  L. C. Pols, <a href="https://doi.org/10.1016/0167-6393(90)90002-Q">"Evaluating text-to-
speech systems: Some methodological aspects,"</a> <i>Speech
Communication,</i> vol. 9, no. 4, pp. 263-270, 1990.</p>

<p>[2] A. S. House, C. Williams, M. H. Hecker, and
  K. D. Kryter, <a href="https://doi.org/10.1121/1.2142744">"Psychoacoustic
    speech tests: A modified rhyme test,"</a> <i>The Journal of the Acoustical Society of America,</i> vol. 35, pp. 1899-1899, 1963.</p>

<p>[3] W. D. Voiers, <a href="">"Evaluating processed speech using the
diagnostic rhyme test,"</a> <i>Speech Technol.,</i> vol. 1, pp. 30-
  39, 1983.</p>

<p>[4] M. Spiegel, M. J. Altom, M. Macchi, and K. Wallace, <a href="https://www.isca-archive.org/sioa_1989/spiegel89_sioa.html">"A monosyllabic test corpus to evaluate the
intelligibility of synthesized and natural speech,"</a> in
<i>Proc. Speech Input/Output Assessment and Speech
Databases,</i> 1989, pp. Vol.2, 5-10. </p> 

<p>[5] U. Jekosch, <a href="https://www.isca-archive.org/sioa_1989/jekosch89_sioa.html">"The cluster-based rhyme test: a segmental synthesis test for open vocabulary,"</a> in <i>Proc. Speech
Input/Output Assessment and Speech Databases,</i> 1989,
pp. Vol.2, 15-18.</p>

<p>[6] J. P. van Santen, <a href="https://www.sciencedirect.com/science/article/pii/S0885230883710041">"Perceptual experiments for diagnostic testing of text-to-speech systems,"</a> <i>Computer
Speech & Language,</i> vol. 7, no. 1, pp. 49-100, 1993.</p>

<p>[7] M. Grice, <a href="https://www.isca-archive.org/sioa_1989/grice89_sioa.html">"Syntactic structures and lexicon requirements for semantically unpredictable sentences in a
number of languages,"</a> in <i>Proc. Speech Input/Output
Assessment and Speech Databases,</i> 1989, pp. Vol.2, 19-
22.</p>

<p>[8] D. Pisoni and S. Hunnicutt, <a href="https://ieeexplore.ieee.org/abstract/document/1170888">"Perceptual evaluation
of MITalk: The MIT unrestricted text-to-speech system,"</a> in <i>ICASSP '80. IEEE International Conference
on Acoustics, Speech, and Signal Processing,</i> vol. 5,
1980, pp. 572-575.</p>

<p>[9] <a href="https://www.itu.int/rec/T-REC-P.800-199608-I">"Methods for subjective determination of transmission
quality,"</a> in <i>ITU-T Rec. P.800.</i> International Telecommunication Union (ITU-R)., 1996.</p>

<p>[10] M. Goldstein, B. Lindstr&#246;m, and O. Till, <a href="https://www.isca-archive.org/icslp_1992/goldstein92b_icslp.html">"Some aspects on
  context and response range effects when assessing naturalness of Swedish sentences generated
by 4 synthesiser systems,"</a> in <i>Proc. 2nd International
Conference on Spoken Language Processing (ICSLP
1992),</i> 1992, pp. 1339-1342.</p>

<p>[11] M. Goldstein, <a href="https://doi.org/10.1016/0167-6393(94)00047-E">"Classification of methods used for
assessment of text-to-speech systems according to
the demands placed on the listener,"</a> <i>Speech Communication,</i> vol. 16, no. 3, pp. 225-244, 1995.</p>


    
    <h4>2.2  Mid-1990s and 2000s: naturalness, intelligibility, and efforts to standardize</h4>

<p>[12] <a href="https://www.itu.int/rec/T-REC-P.85-199406-I/en">"A method for subjective performance assessment of
the quality of speech voice output devices,"</a> in <i>ITU-T
Rec. P.85.</i> International Telecommunication Union
(ITU-R)., 1994.</p>

<p>[13] <a href="https://www.itu.int/rec/T-REC-P.80-199303-S">"Methods for subjective determination of transmission
quality,"</a> in <i>ITU-T Rec. P.80.</i></a> International Telecommunication Union (ITU-R)., 1993.</p>

<p>[14] C. Beno&#238;t, M. Grice, and V. Hazan, <a href="https://doi.org/10.1016/0167-6393(96)00026-X">"The SUS
test: A method for the assessment of text-to-
speech synthesis intelligibility using Semantically
Unpredictable Sentences,"</a> <i>Speech Communication,</i>
vol. 18, no. 4, pp. 381-392, 1996</p>

<p>[15] S. Itahashi, <a href="http://www.lrec-conf.org/proceedings/lrec2000/html/summary/77.htm">"Guidelines for Japanese speech synthesizer evaluation,"</a> in <i>Proceedings of the Second
International Conference on Language Resources and
Evaluation (LREC’00),</i> M. Gavrilidou, G. Carayannis,
S. Markantonatou, S. Piperidis, and G. Stainhauer,
Eds. Athens, Greece: European Language Resources
Association (ELRA), May 2000. </p>

<p>[16] Y. V. Alvarez and
  M. Huckvale, <a href="https://www.isca-archive.org/icslp_2002/alvarez02_icslp.html">"The
    reliability of the ITU-T P.85 standard for the evaluation of
  text-to-speech systems,"</a> in <i>Proc. 7th International Conference
on Spoken Language Processing (ICSLP 2002),</i> 2002,
pp. 329-332.</p>

<p>[17] D. Sityaev, K. Knill, and T. Burrows, <a href="https://www.isca-archive.org/interspeech_2006/sityaev06_interspeech.html">"Comparison
of the ITU-T P.85 standard to other methods for the
evaluation of text-to-speech systems,"</a> in <i>Proc. Interspeech 2006,</i> 2006.</p>

<p>[18]  L. C. W. Pols and U. Jekosch, <a href="https://doi.org/10.1007/978-1-4612-1894-4_41"><i>A Structured
Way of Looking at the Performance of Text-to-Speech Systems.</i></a> New York, NY: Springer New
York, 1997, pp. 519-527</p>

<p>[19] A. W. Black and K. Tokuda, <a href="https://www.isca-archive.org/interspeech_2005/black05_interspeech.html">"The Blizzard Challenge - 2005: evaluating corpus-based speech synthesis on
common datasets,"</a> in <i>Proc. Interspeech 2005,</i> 2005,
pp. 77-80.</p>

<p>[20] N. Campbell, <a href="https://doi.org/10.1007/978-1-4020-5817-2_2"><i>Evaluation of Speech Synthesis.</i></a> Dordrecht: Springer Netherlands, 2007, pp.
29-64</p>

<p>[21] S. Zielinski, F. Rumsey, and S. Bech, <a href="http://www.aes.org/e-lib/browse.cfm?elib=14393">"On some biases
encountered in modern audio quality listening tests - a review,"</a> <i>Journal of the Audio Engineering Society,</i>
vol. 56, no. 6, pp. 427-451, 2008.</p>

    <h4>2.3  2010s to the present: crowdsourcing, MOS, and critiques</h4>

<p>[22] K. Tokuda, H. Zen, and A. W. Black, <a href="https://doi.org/10.1109/WSS.2002.1224415">"An HMM-based
speech synthesis system applied to English,"</a> in <i>IEEE Speech Synthesis Workshop.</i> IEEE Santa Monica,
2002, pp. 227-230.</p>

<p>[23] X. Wang, J. Yamagishi, M. Todisco, H. Delgado,
A. Nautsch, N. Evans, M. Sahidullah, V. Vestman, T. Kinnunen, K. A. Lee, L. Juvela, P. Alku,
Y.-H. Peng, H.-T. Hwang, Y. Tsao, H.-M. Wang,
S. L. Maguer, M. Becker, F. Henderson, R. Clark,
Y. Zhang, Q. Wang, Y. Jia, K. Onuma, K. Mushika,
T. Kaneda, Y. Jiang, L.-J. Liu, Y.-C. Wu, W.-C.
Huang, T. Toda, K. Tanaka, H. Kameoka, I. Steiner,
D. Matrouf, J.-F. Bonastre, A. Govender, S. Ronanki, J.-X. Zhang, and
  Z.-H. Ling, <a href="https://doi.org/10.1016/j.csl.2020.101114">"ASVspoof 2019:
A large-scale public database of synthesized, converted and replayed speech,"</a> <i>Computer Speech &
Language,</i> vol. 64, p. 101114, 2020. </p>

<p>[24]  F. Ribeiro, D. Florêncio, C. Zhang, and M. Seltzer, 
  <a href="https://doi.org/10.1109/ICASSP.2011.5946971">CrowdMOS: An approach for crowdsourcing mean opinion score studies," </a>
  in 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 
  IEEE, 2011, pp. 2416–2419.</p>

<p>[25] S. Buchholz, J. Latorre, and K. Yanagisawa, 
  <a href="https://doi.org/10.1002/9781118541241.ch7">"Crowdsourced assessment of speech synthesis,"</a> 
  in Crowdsourcing for Speech Processing: Applications to Data, Collection, 
  Transcription and Assessment, M. Eskénazi, G.-A. Levow, H. Meng, G. Parent, 
  and D. Suendermann, Eds. Chichester: John Wiley & Sons, Ltd, 2013, ch. 7, pp. 173–214.</p>

<p>[26] M. Wester, C. Valentini-Botinhao, and G. E. Henter, 
 <a href="https://doi.org/10.21437/Interspeech.2015-689">"Are we using enough listeners? No!—An empirically supported critique of interspeech 2014 TTS evaluations,"</a>
  in Proc. Interspeech 2015, 2015, pp. 3476–3480.</p>

<p>[27] <a href="https://www.itu.int/rec/R-REC-BS.1534-3-201510-I">"Method for the Subjective Assessment of Intermediate Sound Quality (MUSHRA),"</a>
  in Recommendation ITU-R BS.1534-3. International Telecommunication Union (ITU-R)., 2015.</p>

<p>[28] R. C. Streijl, S. Winkler, and D. S. Hands, 
  <a href="https://link.springer.com/article/10.1007/s00530-014-0446-1">"Mean opinion score (MOS) revisited: methods and applications, limitations and alternatives,"</a>
  Multimedia Systems, vol. 22, no. 2, pp. 213–227, 2016.</p>

<p>[29] P. Wagner, J. Beskow, S. Betz, J. Edlund, J. Gustafson, G. Eje Henter, S. Le Maguer, Z. Malisz, 
  Éva Székely, C. Tånnander, and J. Voße, 
  <a href="https://doi.org/10.21437/SSW.2019-19">"Speech Synthesis Evaluation — State-of-the-Art Assessment 
  and Suggestion for a Novel Research Program,"</a>
  in Proc. 10th ISCA Workshop on Speech Synthesis (SSW 10), 
  2019, pp. 105–110.</p>

<p>[30] M. Wester, O. Watts, and G. E. Henter, 
  <a href="https://doi.org/10.21437/SpeechProsody.2016-157">"Evaluating comprehension of natural and synthetic conversational speech,"</a>
  in Proc. Speech Prosody 2016, 2016, pp. 766–770.</p>

<p>[31] J. Mendelson and M. P. Aylett, 
  <a href="https://doi.org/10.21437/Interspeech.2017-1438">"Beyond the Listening Test: An Interactive Approach to TTS Evaluation,"</a>
  in Proc. Interspeech 2017, 2017, pp. 249–253.</p>

<p>[32] R. Clark, H. Silen, T. Kenter, and R. Leith, 
  <a href="https://doi.org/10.21437/SSW.2019-18">"Evaluating Long-form Text-to-Speech: Comparing the Ratings of Sentences and Paragraphs,"</a>
  in Proc. 10th ISCA Workshop on Speech Synthesis (SSW 10), 2019, pp. 99–104.</p>

<p>[33] J. O'Mahony, P. O.Gallegos, C. Lai, and S. King, 
  <a href="https://doi.org/10.21437/SSW.2021-26">"Factors affecting the evaluation of synthetic speech in context,"</a> 
  in The 11th ISCA Speech Synthesis Workshop (SSW11). International 
  Speech Communication Association, 2021, pp. 148–153.</p>

<p>[34] R. Dall, J. Yamagishi, and S. King, 
  <a href="https://doi.org/10.21437/SpeechProsody.2014-192">"Rating naturalness in speech synthesis: The effect of style and expectation,"</a>
  in Proc. 7th International Conference on Speech Prosody 2014, 2014, pp. 1012–1016.</p>

<p>[35] S. Shirali-Shahreza and G. Penn, 
  <a href="https://doi.org/10.21437/SSW.2023-31">"Better Replacement for TTS Naturalness Evaluation,"</a> 
  in Proc. 12th ISCA Speech Synthesis Workshop (SSW2023), 2023, pp.197–203.</p>

<p>[36] S. King, 
  <a href="https://doi.org/10.3989/loquens.2014.006">"Measuring a decade of progress in text-to-speech,"</a> 
  Loquens, vol. 1, no. 1, p. e006, 2014.</p>

<p>[37] F. Seebauer, M. Kuhlmann, R. Haeb-Umbach, and P. Wagner, 
  <a href="https://doi.org/10.21437/SSW.2023-6">"Re-examining the quality dimensions of synthetic speech,"</a> 
  in Proc. 12th ISCA Speech Synthesis Workshop (SSW2023), 2023, pp. 34–40.</p>

<p>[38] S. Shirali-Shahreza and G. Penn, 
  <a href="https://doi.org/10.1109/SLT.2018.8639599">"MOS Naturalness and the Quest for Human-Like Speech,"</a>
  in 2018 IEEE Spoken Language Technology Workshop (SLT), 2018, pp. 346–352.</p>

<p>[39] J. Camp, T. Kenter, L. Finkelstein, and R. Clark, 
  <a href="https://doi.org/10.21437/Interspeech.2023-2014">"MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors,"</a>
  in Proc. INTERSPEECH 2023, 2023, pp. 1090–1094.</p>

<p>[40]  Y. Yasuda and T. Toda, 
  <a href="https://doi.org">Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech Based on Tail Probabilities,"</a>
  in Proc. INTERSPEECH 2023, 2023, pp. 5491–5495.</p>

<p>[41] E. Cooper and J. Yamagishi, 
  <a href="https://doi.org/10.21437/Interspeech.2023-1076">"Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech,"</a>
  in Proc. INTERSPEECH 2023, 2023, pp. 1104–1108.</p>

<p>[42] A. Kirkland, S. Mehta, H. Lameris, G. E. Henter, E. Szekely, and J. Gustafson, 
  <a href="https://doi.org/10.21437/SSW.2023-7">"Stuck in the MOS pit: A critical analysis of MOS test methodology in TTS evaluation,"</a>
  in Proc. 12th ISCA Speech Synthesis Workshop (SSW2023), 2023, pp. 41–47.</p>

<p>[43] S. Le Maguer, S. King, and N. Harte, 
  <a href="https://doi.org/10.1016/j.csl.2023.101577">"The limits of the mean opinion score for speech synthesis evaluation,"</a>
  Computer Speech & Language, vol. 84, p. 101577, 2024. </p>
    
  <h3>3. Automatic evaluation for synthetic speech</h3>

<p>[44] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, 
  <a href="https://doi.org/10.3115/1073083.1073135">"BLEU: a Method for Automatic Evaluation of Machine Translation,"</a>
  in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311–318.</p>
  
    <h4>3.1  Difficulties in automatic evaluation of synthetic speech</h4>

    <h4>3.2  Speech quality assessment metrics from telephony</h4>

<p>[45] R. Kubichek, 
  <a href="https://doi.org/10.1109/PACRIM.1993.407206">"Mel-cepstral distance measure for objective speech quality assessment,"</a>
  in Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing, vol. 1, 1993, pp. 125–128 vol.1.</p>

<p>[46] J. Kominek, T. Schultz, and A. W. Black, 
  <a href="https://www.isca-archive.org/sltu_2008/kominek08_sltu.html">"Synthesizer voice quality of new languages calibrated with mean Mel cepstral distortion,"</a>
  in Proc. Speech Technology for Under-Resourced Languages (SLTU-2008), 2008, pp. 63–68.</p>

<p>[47] <a href="https://handle.itu.int/11.1002/1000/5374">"Perceptual evaluation of speech quality (PESQ): 
  An objective method for end-to-end speech quality assessment of narrow-band telephone networks 
  and speech codecs,"</a> in ITU-T Recommendation P.862, 2001.</p>

<p>[48] S. Ipswich, "PESQ: An Introduction White Paper," 2001.</p>

<p>[49] M. Cernak and M. Rusko, 
  <a href="http://www.conforg.fr/acoustics2008/cdrom/data/fa2005-budapest/paper/334-0.pdf">"An evaluation of synthetic speech using the PESQ measure,"</a>
  in Proc. European Congress on Acoustics, 2005, pp. 2725–2728.</p>

<p>[50] F. Hinterleitner, S. Zabel, S. Möller, L. Leutelt, and C. Norrenbrock, 
  <a href="https://www.essv.de/pdf/2011_99_106.pdf">"Predicting the quality of synthesized speech using reference-based prediction measures,"</a>
  in Konferenz Elektronische Sprachsignalverarbeitung. TUDpress, Dresden, 2011, pp. 99–106.</p>

<p>[51] L. Latacz and W. Verhelst, 
  <a href="https://doi.org/10.21437/Interspeech.2015-691">"Double-ended prediction of the naturalness ratings of 
  the Blizzard Challenge 2008-2013,"</a> in Proc. Interspeech 2015, 2015, pp. 3486–3490.
</p>

<p>[52] <a href="https://www.itu.int/rec/T-REC-P.563/en">"Single-ended method for objective speech quality assessment in narrow-band telephony applications,"</a>
  in ITU-T Rec. P.563, 2004.</p>

<p>[53] L. Malfait, J. Berger, and M. Kastner, 
  <a href="https://doi.org/10.1109/TASL.2006.883177">"P. 563—The ITU-T standard for single-ended speech quality assessment,"</a>
  IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 6, pp. 1924–1934, 2006.</p>

<p>[54] D.-S. Kim and A. Tarraf, 
  <a href="https://doi.org/10.1002/bltj.20228">"Anique+: A new American national standard for non-intrusive 
  estimation of narrowband speech quality,"</a> Bell Labs Technical Journal, vol. 12, no. 1, pp. 221–236, 
  2007.</p>

<p>[55] T. H. Falk, S. Möller, V. Karaiskos, and S. King, 
  <a href="http://www.festvox.org/blizzard/bc2008/tlabs_Blizzard2008.pdf">"Improving instrumental quality prediction performance for the Blizzard Challenge,"</a>
  in Proc. Blizzard Challenge Workshop, 2008.</p>

<p>[56] T. H.Falk and S. Moller, 
  <a href="https://doi.org/10.1109/LSP.2008.2006709">"Towards signal-based instrumental quality diagnosis for text-to-speech 
  systems,"</a>
  IEEE Signal Processing Letters, vol. 15, pp. 781–784, 2008.</p>

<p>[57] T. Yoshimura, G. E. Henter, O. Watts, M. Wester, J. Yamagishi, and K. Tokuda, 
  <a href="https://doi.org/10.21437/Interspeech.2016-847">"A Hierarchical Predictor of Synthetic Speech Naturalness Using Neural Networks,"</a>
  in Proc. Interspeech 2016, 2016, pp. 342–346.</p>

<p>[58] R. Clark and K. Dusterhoff, 
  <a href="https://doi.org/10.21437/Eurospeech.1999-368">"Objective methods for evaluating synthetic intonation,"</a>
  in Proc. Eurospeech '99, Sixth European Conf. on Speech Communication and Technology, Budapest, 
  Hungary, 1999, pp. 1623—-1626.</p>

<p>[59] U. Remes, R. Karhila, and M. Kurimo, 
  <a href="https://www.isca-archive.org/ssw_2013/remes13_ssw.html">"Objective evaluation measures for speaker-adaptive HMM-TTS systems,"</a>
  in Eighth ISCA Workshop on Speech Synthesis, 2013.</p>

    
    <h4>3.3  Models for evaluation of synthetic speech</h4>

<p>[60] F. Hinterleitner, S. Zander, K.-P. Engelbrecht, and S. Möller, 
  <a href="https://www.essv.de/paper.php?id=361">"On the use of automatic speech recognizers for the quality and intelligibility prediction of 
  synthetic speech,"</a> in Konferenz Elektronische Sprachsignalverarbeitung. TUDpress, Dresden, 2015, pp. 
  105–111.</p>

<p>[61] O. Sharoni, R. Shenberg, and E. Cooper, 
  <a href="https://doi.org/10.21437/Interspeech.2023-430">"SASPEECH: A Hebrew single speaker dataset for text to speech and voice conversion,"</a>
  in Proc. Interspeech, 2023.</p>

<p>[62] S. Mehta, R. Tu, J. Beskow, É. Székely, and G. E. Henter, 
  <a href="https://doi.org/10.1109/ICASSP48485.2024.10448291">"Matcha-TTS: A fast TTS architecture with conditional flow matching,"</a>
  in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing 
  (ICASSP) (to appear), 2024.</p>

<p>[63]  W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang, J. Raiman, and J. Miller, 
  <a href="https://openreview.net/forum?id=HJtEm4p6Z">"Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,"</a>
  in International Conference on Learning Representations, 2018.
</p>
    
    
      <h5>3.3.1  Early attempts at machine learning based synthetic speech quality prediction</h5>

<p>[64] F. Hinterleitner, S. Möller, T. H. Falk, and T. Polzehl, 
  <a href="http://www.festvox.org/blizzard/bc2010/DeutcheTelekom_Blizzard2010.pdf">"Comparison of approaches for instrumentally predicting the quality of text-to-speech systems: 
  Data from Blizzard Challenges 2008 and 2009,"</a>
  in Blizzard Challenge Workshop, vol. 2010, 2010, pp. 48–60.</p>

<p>[65] V. Karaiskos, S. King, R. A. Clark, and C. Mayo, 
  <a href="http://festvox.org/blizzard/bc2008/summary_Blizzard2008.pdf">"The Blizzard Challenge 2008,"</a>
  in Proc. Blizzard Challenge Workshop. Citeseer, 2008.</p>

<p>[66] A. W. Black, S. King, and K. Tokuda, 
  <a href="http://www.festvox.org/blizzard/bc2009/summary_Blizzard2009.pdf">"The Blizzard Challenge 2009,"</a>
  in Proc. Blizzard Challenge, 2009, pp. 1–24.</p>

<p>[67] S. King and V. Karaiskos, 
  <a href="http://www.festvox.org/blizzard/bc2011/summary_Blizzard2011.pdf">"The Blizzard Challenge 2011,"</a> 2012.</p>

<p>[68] C. R. Norrenbrock, F. Hinterleitner, U. Heute, and S. Möller, 
  <a href="http://www.festvox.org/blizzard/bc2012/Norrenbrock_etal_Blizzard_workshop_2012_final.pdf">"Towards perceptual quality modeling of synthesized audiobooks – Blizzard Challenge 2012,"</a>
  in Blizzard Challenge Workshop, 2012.</p>


      <h5>3.3.2  Neural network-based synthetic speech quality prediction</h5>

<p>[69] T. Toda, L.-H. Chen, D. Saito, F. Villavicencio, M. Wester, Z. Wu, and J. Yamagishi, 
  <a href="https://doi.org/10.21437/Interspeech.2016-1066">"The Voice Conversion Challenge 2016,"</a> in Proc. Interspeech, 2016, pp. 1632–1636.</p>

<p>[70] J. Lorenzo-Trueba, J. Yamagishi, T. Toda, D. Saito, F. Villavicencio, T. Kinnunen, 
  and Z. Ling, 
  <a href="https://www.isca-archive.org/odyssey_2018/lorenzotrueba18_odyssey.pdf">"The Voice Conversion Challenge 2018: Promoting Development of Parallel and 
  Nonparallel Methods,"</a> in Proc. Odyssey, 2018, pp. 195–202.</p>

<p>[71]  Y. Zhao, W.-C. Huang, X. Tian, J. Yamagishi, R. K. Das, T. Kinnunen, Z. Ling, and T. Toda, 
  <a href="https://doi.org/10.21437/VCCBC.2020-14">"Voice Conversion Challenge 2020 – Intra-lingual semi-parallel and cross-lingual voice conversion,"</a> 
in Proc. Joint Workshop for the BC and VCC 2020, 2020, pp. 80–98.</p>

<p>[72] W.-C.Huang, L. P. Violeta, S. Liu, J. Shi, and T. Toda, 
  <a href="https://doi.org/10.1109/ASRU57964.2023.10389671">"The Singing Voice Conversion Challenge 2023,"</a> in Proc. ASRU, 2023.</p>

<p>[73] J. Williams, J. Rownicka, P. Oplustil, and S. King, 
  <a href="https://doi.org/10.21437/Odyssey.2020-32">"Comparison of Speech Representations for Automatic Quality Estimation in Multi-Speaker Text-to-Speech
  Synthesis,"</a>
  in Proc. The Speaker and Language Recognition Workshop (Odyssey 2020), 2020, pp. 222–229.</p>

<p>[74]  B. Patton, Y. Agiomyrgiannakis, M. Terry, K. Wilson, R. A. Saurous, and D. Sculley, 
  <a href="https://arxiv.org/abs/1611.09207">"AutoMOS: Learning a non-intrusive assessor of naturalness-of-speech,"</a>
  arXiv preprint arXiv:1611.09207, 2016.</p>

<p>[75] C.-C. Lo, S.-W. Fu, W.-C. Huang, X. Wang, J. Yamagishi, Y. Tsao, and H.-M. Wang,
  <a href="https://doi.org/10.21437/Interspeech.2019-2003">"MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion,"</a> 
  in Proc. Interspeech 2019, 2019, pp. 1541–1545.</p>

<p>[76] S.-W. Fu, Y. Tsao, H.-T. Hwang, and H.-M. Wang, 
<a href="https://doi.org/10.21437/Interspeech.2018-1802">"Quality-Net: An End-to-End Non-intrusive Speech Quality Assessment Model Based on BLSTM,"</a> 
  in Proc. Interspeech 2018, 2018, pp. 1873–1877.</p>

<p>[77] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, 
  <a href="https://doi.org/10.1109/ICASSP.2018.8461375">"X-Vectors: Robust DNN embeddings for speaker recognition,"</a> in 2018 
  IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 5329–5333.</p>

<p>[78] Y. Choi, Y. Jung, and H. Kim, 
  <a href="https://doi.org/10.21437/Interspeech.2020-2111">"Deep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling,"</a>
  in Proc. Interspeech 2020, 2020, pp. 1743–1747.</p>

<p>[79] Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg, J. Shor, Y. Xiao, Y. Jia, F. Ren, and
  R. A. Saurous, 
  <a href="https://proceedings.mlr.press/v80/wang18h.html">"Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,"</a> 
  in International conference on machine learning. PMLR, 2018, pp. 5180–5189.</p>

      <h5>3.3.3  Listener modeling in synthetic speech evaluation</h5>

<p>[80] Y. Leng, X. Tan, S. Zhao, F. Soong, X.-Y. Li, and T. Qin, 
  <a href="https://doi.org/10.1109/ICASSP39728.2021.9413877">"MBNet: MOS prediction for synthesized speech with mean-bias network,"</a> in ICASSP 2021-2021 
  IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 391–395.</p>

<p>[81] W.-C. Huang, E. Cooper, J. Yamagishi, and T. Toda, 
  <a href="https://doi.org/10.1109/ICASSP43922.2022.9747222">"LDNet: Unified listener dependent modeling in MOS prediction for synthetic speech,"</a> in ICASSP 2022-2022 
  IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 896–900.</p>

  <p>[82] X. Liang, F. Cumlin, C. Schüldt, and S. Chatterjee, 
    <a href="https://doi.org/10.21437/Interspeech.2023-1436">"DeePMOS: Deep Posterior Mean-Opinion-Score of Speech,"</a> in Proc. INTERSPEECH 2023, 2023, pp. 526–530.
    </p>

    <h5>3.3.4  SSL-based approaches</h5>

<p>[83] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, 
  <a href="https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html">"wav2vec 2.0: A framework for self-supervised learning of speech representations,"</a> 
  Advances in neural information processing systems, vol. 33, pp. 12 449–12 460, 2020.</p>

<p>[84]  A. Mohamed, H.-Y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li,
  K. Livescu, L. Maaløe, T. N. Sainath, and S. Watanabe, 
  <a href="https://doi.org/10.1109/JSTSP.2022.3207050">"Self-Supervised Speech Representation Learning: A Review,"</a> 
  IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1179–1210, 2022.</p>

<p>[85] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, 
  <a href="https://doi.org/10.1109/TASLP.2021.3122291">"HuBERT: Self supervised speech representation learning by masked prediction of hidden units,"</a> 
  IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021.</p>

<p>[86] S. W. Yang, P. H. Chi, Y. S. Chuang, C. I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, 
  G. T. Lin et al., 
  <a href="https://doi.org/10.21437/Interspeech.2021-1775">"SUPERB: Speech processing Universal PERformance Benchmark,"</a> in 22nd Annual Conference of the International Speech Communication Association, 
  INTERSPEECH 2021. International Speech Communication Association, 2021, pp. 3161–3165.</p>

<p>[87] W.-C. Tseng, C.-Y. Huang, W.-T. Kao, Y. Y. Lin, and H.-Y. Lee, 
  <a href="https://doi.org/10.21437/Interspeech.2021-2013">"Utilizing Self-Supervised Representations for MOS Prediction,"</a>
  in Proc. Interspeech 2021, 2021, pp. 2781–2785.</p>

<p>[88] E. Cooper, W.-C. Huang, T. Toda, and J. Yamagishi, 
  <a href="https://doi.org/10.1109/ICASSP43922.2022.9746395">"Generalization ability of MOS prediction networks,"</a> in ICASSP 2022-2022 
  IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8442–8446.</p>

<p>[89] A. Vioni, G. Maniati, N. Ellinas, J. S. Sung, I. Hwang, A. Chalamandaris, and P. Tsiakoulis, 
  <a href="https://doi.org/10.1109/ICASSP49357.2023.10096255">"Investigating Content-Aware Neural Text-to-Speech MOS Prediction Using Prosodic and Linguistic Features,"</a>
  in ICASSP 2023-2023 
  IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1–5.</p>

<p>[90] G. Maniati, A. Vioni, N. Ellinas, K. Nikitaras, K. Klapsas, J. S. Sung, G. Jho, A. Chalamandaris, and 
  P. Tsiakoulis, 
  <a href="https://doi.org/10.21437/Interspeech.2022-10922">"SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis,"</a> 
  in Proc. Interspeech, 2022, pp. 2388–2392.</p>

<p>[91] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous, 
  <a href="https://doi.org/10.21437/Interspeech.2017-1452">"Tacotron: Towards End-to-End Speech Synthesis,"</a> in Proc. Interspeech 2017, 2017, pp. 4006–4010.</p>

<p>[92] J. D. M.-W. C. Kenton and L. K. Toutanova, 
  <a href="https://doi.org/10.18653/v1/N19-1423">"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"</a> 
  in Proceedings of NAACL HLT, 2019, pp. 4171–4186.</p>

<p>[93] H. Wang, S. Zhao, X. Zheng, and Y. Qin, 
  <a href="https://doi.org/10.21437/Interspeech.2023-851">"RAMP: Retrieval-Augmented MOS Prediction via Confidencebased Dynamic Weighting,"</a> 
  in Proc. INTERSPEECH 2023, 2023, pp. 1095–1099.</p>

 <p>[94] T. Sellam, A. Bapna, J. Camp, D. Mackinnon, A. P. Parikh, and J. Riesa, 
   <a href="https://doi.org/10.1109/ICASSP49357.2023.10094909">"SQuId: Measuring speech naturalness in many languages,"</a> in ICASSP 2023-2023 
   IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1–5.</p>
  
      <h5>3.3.5  Unsupervised approaches for synthetic speech quality prediction</h5>

<p>[95] V. Hodge and J. Austin, 
  <a href="https://doi.org/10.1023/B:AIRE.0000045502.10941.a9">"A survey of outlier detection methodologies,"</a> Artificial intelligence review, vol. 22, pp. 85–126, 2004.</p>

<p>[96] S. L. Maguer, N. Barbot, and O. Boeffard, 
  <a href="https://www.isca-archive.org/ssw_2013/maguer13_ssw.html">"Evaluation of contextual descriptors for HMM-based speech synthesis in French,"</a> 
  in Eighth ISCA Workshop on Speech Synthesis, 2013.</p>

<p>[97] C. T. Do, M. Evrard, A. Leman, C. d’Alessandro, A. Rilliard, and J. L. Crebouw, 
  <a href="https://doi.org/10.21437/Interspeech.2014-203">"Objective evaluation of HMM-based speech synthesis system using Kullback-Leibler divergence,"</a> 
  in Proc. Interspeech 2014, 2014, pp. 2952–2956.</p>

<p>[98] S. Maiti, Y. Peng, T. Saeki, and S. Watanabe, 
  <a href="https://doi.org/10.1109/ICASSP49357.2023.10095710">"SpeechLMScore: Evaluating speech generation using speech language model,"</a> in ICASSP 2023-2023 
  IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1–5.</p>

<p>[99] A. Ravuri, E. Cooper, and J. Yamagishi, 
  <a href="https://arxiv.org/abs/2312.15616">"Uncertainty as a predictor: Leveraging self-supervised learning for zero-shot MOS prediction,"</a>
  IEEE ICASSP 2024 workshop on Self-supervision in Audio, Speech and Beyond, 2024.</p>

  
<p>[100] S. Schneider, A. Baevski, R. Collobert, and M. Auli, 
  <a href="https://doi.org/10.21437/Interspeech.2019-1873">"wav2vec: Unsupervised Pre-Training for Speech Recognition,"</a> 
  in Proc. Interspeech 2019, 2019, pp. 3465–3469.</p>
  
      <h5>3.3.6  Beyond predicting quality of synthesized speech</h5>

<p>[101] E. Klabbers and R. Veldhuis, 
  <a href="https://doi.org/10.21437/ICSLP.1998-31">"On the reduction of concatenation artefacts in diphone synthesis,"</a>
  in Proc. 5th International Conference on Spoken Language Processing (ICSLP 1998), 1998, p. paper 0115.</p>

<p>[102] Y. Stylianou and A. K. Syrdal, 
  <a href="https://doi.org/10.1109/ICASSP.2001.941045">"Perceptual and objective detection of discontinuities in 
  concatenative speech synthesis,"</a> in 2001 IEEE international conference on acoustics, speech, and 
  signal processing. proceedings (Cat. No. 01CH37221), vol. 2. IEEE, 2001, pp. 837–840.</p>

<p>[103] J. Vepa, S. King, and P. Taylor, 
  <a href="https://doi.org/10.21437/ICSLP.2002-663">"Objective distance measures for spectral discontinuities in concatenative speech synthesis,"</a>
  in ICSLP 2002: 7th International Conference on Spoken Language Processing. International Speech 
  Communication Association, 2002, pp. 2605–2608.</p>

<p>[104] M. Lee, 
  <a href="https://www.isca-archive.org/eurospeech_2001/lee01f_eurospeech.pdf">"Perceptual cost functions for unit searching in large corpus-based concatenative text-to-speech,"</a> 
  in Proc. EUROSPEECH, Aalborg, Denmark, 2001, pp. 2227–2230.</p>

<p>[105] T. Toda, H. Kawai, M. Tsuzaki, and K. Shikano, 
  <a href="https://doi.org/10.1109/WSS.2002.1224404">"Perceptual evaluation of cost for segment selection in concatenative speech synthesis,"</a>
  in Proceedings of 2002 IEEE Workshop on Speech Synthesis, 2002. IEEE, 2002, pp. 183–186.</p>

<p>[106] L. Formiga and F. Alías, 
  <a href="https://doi.org/10.1007/978-3-540-73007-1_79">"Extracting user preferences by GTM for AiGA weight tuning in unit selection text-to-speech synthesis,"</a>
  in International Work-Conference on Artificial Neural Networks. Springer, 2007, pp. 654–661.</p>

<p>[107] C. Nakatsu and M. White, 
  <a href="https://doi.org/10.3115/1220175.1220315">"Learning to say it well: Reranking realizations by predicted synthesis quality,"</a> 
  in Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, 2006, pp. 1113–1120.</p>

<p>[108]  Y. Choi, Y. Jung, Y. Suh, and H. Kim, 
  <a href="https://doi.org/10.1109/ACCESS.2022.3175810">"Learning to Maximize Speech Quality Directly Using MOS Prediction for Neural Text-to-Speech,"</a> 
  IEEE Access, vol. 10, pp. 52 621–52 629, 2022.</p>

<p>[109] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, 
  <a href="https://doi.org/10.1609/aaai.v33i01.33016706">"Neural speech synthesis with transformer network,"</a> 
  in Proceedings of the AAAI conference on artificial intelligence, vol. 33, no. 01, 2019, pp. 6706–6713.</p>

<p>[110] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari, 
  <a href="https://doi.org/10.1109/ICASSP49357.2023.10095161">"Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection,"</a>
  in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.</p>

<p>[111] C.-H. Hu, Y.-H. Peng, J. Yamagishi, Y. Tsao, and H.-M. Wang, 
  <a href="https://doi.org/10.1109/LSP.2022.3152672">"SVSNet: An End-to-End Speaker Voice Similarity Assessment Model,"</a> 
  IEEE Signal Processing Letters, vol. 29, pp. 767–771, 2022.</p>

<p>[112] M. Ravanelli and Y. Bengio, 
  <a href="https://doi.org/10.1109/SLT.2018.8639585">"Speaker recognition from raw waveform with SincNet,"</a> 
  in 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018, pp. 1021–1028.</p>

<p>[113] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen, R. Pang, I. Lopez Moreno, 
  Y. Wu et al., 
  <a href="https://proceedings.neurips.cc/paper/2018/hash/6832a7b24bc06775d02b7406880b93fc-Abstract.html">"Transfer learning from speaker verification to multispeaker text-to-speech synthesis,"</a>
  Advances in Neural Information Processing Systems, vol. 31, 2018.</p>

<p>[114]  E. Casanova, C. Shulby, E. Gölge, N. M. Müller, F. S. de Oliveira, A. Candido Jr., 
  A. da Silva Soares, S. M. Aluisio, and M. A. Ponti, 
  <a href="https://doi.org/10.21437/Interspeech.2021-1774">"SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker Text-To-Speech Model,"</a> 
  in Proc. Interspeech 2021, 2021, pp. 3645–3649.</p>

<p>[115] R. K. Das, T. Kinnunen, W.-C. Huang, Z.-H. Ling, J. Yamagishi, Z. Yi, X. Tian, and T. Toda, 
  <a href="https://doi.org/10.21437/VCCBC.2020-15">"Predictions of Subjective Ratings and Spoofing Assessments of Voice Conversion Challenge 2020 Submissions,"</a> 
  in Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020, 2020, pp. 99–120.</p>

<p>[116] Y. Choi, Y. Jung, and H. Kim, 
  <a href="https://doi.org/10.1109/SLT48900.2021.9383533">"Neural MOS prediction for synthesized speech using multi-task learning with spoofing detection and 
  spoofing type classification,"</a> in 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021, pp. 
  462–469.</p>

<p>[117]  W. Zhou, Z. Yang, C. Chu, S. Li, R. Dabre, Y. Zhao, and T. Kawahara, 
  <a href="https://doi.org/10.1109/ICASSP48485.2024.10446041">"MOS-FAD: Improving Fake Audio Detection Via Automatic Mean Opinion Score Prediction,"</a>
  in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (to appear). IEEE, 2024.</p>

      <h5>3.3.7  Predicting rank order and pairwise preferences</h5>


<p>[118] G. Mittag, S. Zadtootaghaj, T. Michael, B. Naderi, and S. Möller, 
  <a href="https://doi.org/10.1109/QoMEX51781.2021.9465384">"Bias-aware loss for training image and speech quality prediction models from multiple datasets,"</a> 
  in 2021 13th International Conference on Quality of Multimedia Experience (QoMEX). IEEE, 2021, pp. 97–102.</p>

<p>[119] H. Yadav, E. Cooper, J. Yamagishi, S. Sitaram, and R. R. Shah, 
  <a href="https://doi.org/10.1109/ASRU57964.2023.10389797">"Partial Rank Similarity Minimization Method for Quality MOS Prediction of Unseen Speech Synthesis</a> 
  Systems in Zero-Shot and Semi-Supervised Setting," in 2023 IEEE Automatic Speech Recognition and 
  Understanding Workshop (ASRU). IEEE, 2023, pp. 1–7.</p>

<p>[120] C. Valentini-Botinhao, M. S. Ribeiro, O. Watts, K. Richmond, and G. E. Henter, 
  <a href="https://www.isca-archive.org/interspeech_2022/valentinibotinhao22_interspeech.html">"Predicting pairwise preferences between TTS audio stimuli using parallel ratings data and 
  anti-symmetric twin neural networks,"</a> in Proc. Interspeech 2022, 2022, pp. 471–475.</p>
  
<p>[121] C.-H. Hu, Y. Yasuda, and T. Toda, 
  <a href="https://www.isca-archive.org/interspeech_2023/hu23d_interspeech.html">"Preference-based training framework for automatic speech quality assessment using deep neural network,"</a> 
  in Proc. INTERSPEECH 2023, 2023, pp. 546–550.</p>
      
      <h5>3.3.8  Learning from speech quality prediction in other domains</h5>

<p>[122] C. K. Reddy, V. Gopal, and R. Cutler, 
  <a href="https://doi.org/10.1109/ICASSP39728.2021.9414878">"DNSMOS: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors,"</a> 
  in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 
  IEEE, 2021, pp. 6493–6497.</p>

<p>[123] R. E. Zezario, S.-W. Fu, F. Chen, C.-S. Fuh, H.-M. Wang, and Y. Tsao, 
  <a href="https://doi.org/10.1109/TASLP.2022.3205757">"Deep learning-based non-intrusive multi-objective speech assessment model with cross-domain features,"</a>
  IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 54–70, 2022.</p>

<p>[124] P. Manocha, B. Xu, and A. Kumar, 
  <a href="https://proceedings.neurips.cc/paper/2021/hash/bc6d753857fe3dd4275dff707dedf329-Abstract.html">"NORESQA: A framework for speech quality assessment using nonmatching references,"</a> 
  Advances in Neural Information Processing Systems, vol. 34, pp. 22 363–22 378, 2021.</p>

<p>[125] G. Mittag and S. Möller, 
  <a href="https://doi.org/10.21437/Interspeech.2020-2382">"Deep Learning Based Assessment of Synthetic Speech Naturalness,"</a>
  in Proc. Interspeech 2020, 2020, pp. 1748–1752.</p>      

<p>[126] G. Mittag and S. Möller, 
  <a href="https://doi.org/10.1109/ICASSP40776.2020.9053951">"Full-reference speech quality estimation with attentional Siamese neural networks,"</a> 
  in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 346–350.</p>
      
  <h3>4. The VoiceMOS Challenge 2022</h3>


<p>[127] W. C. Huang, E. Cooper, Y. Tsao, H.-M. Wang, T. Toda, and J. Yamagishi, 
  <a href="https://doi.org/10.21437/Interspeech.2022-970">"The VoiceMOS Challenge 2022,"</a> 
  in Proc. Interspeech, 2022, pp. 4536–4540.</p>
  
  
    <h4>4.1  Data and tracks</h4>


<p>[128]  S. King and V. Karaiskos, 
  <a href="http://www.festvox.org/blizzard/bc2010/summary_Blizzard2010.pdf">"The Blizzard Challenge 2010,"</a> 2010.</p>

<p>[129] S. King and V. Karaiskos, 
  <a href="http://www.festvox.org/blizzard/bc2011/summary_Blizzard2011.pdf">"The Blizzard Challenge 2011,"</a> 2011.
</p>

<p>[130] S. King and V. Karaiskos, 
  <a href="http://www.festvox.org/blizzard/bc2013/summary_Blizzard2013.pdf">"The Blizzard Challenge 2013,"</a> 2013.</p>

<p>[131] S. King and V. Karaiskos, 
  <a href="http://www.festvox.org/blizzard/bc2016/blizzard2016_overview_paper.pdf">"The Blizzard Challenge 2016,"</a> 2016.</p>

<p>[132] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, 
  Y. Zhang, and X. Tan, 
  <a href="https://doi.org/10.1109/ICASSP40776.2020.9053512">"ESPnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit,"</a> in Proc. ICASSP 2020.</p>

<p>[133]  E. Cooper and J. Yamagishi, 
  <a href="https://doi.org/10.21437/SSW.2021-32">"How do voices from past speech synthesis challenges compare today?"</a> 
  in Proc. SSW, 2021, pp. 183–188.</p>


    <h4>4.2  Baselines</h4>
    
    <h4>4.3  Team approaches</h4>


<p>[134] T. Saeki, D. Xin, W. Nakata, T. Koriyama, S. Takamichi, and H. Saruwatari, 
  <a href="https://doi.org/10.21437/Interspeech.2022-439">"UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022,"</a> 
  in Proc. Interspeech, 2022, pp. 4521–4525.</p>

<p>[135] W.-C. Tseng, W.-T. Kao, and H.-Y. Lee, 
  <a href="https://doi.org/10.21437/Interspeech.2022-11247">"DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution 
  of Opinion Scores,"</a> in Proc. Interspeech, 2022, pp. 4541–4545.</p>

<p>[136] A. Stan, 
  <a href="https://doi.org/10.21437/Interspeech.2022-105">"The ZevoMOS entry to VoiceMOS Challenge 2022,"</a> in Proc. Interspeech, 2022, pp. 4516–4520.</p>

<p>[137] R. Reimao and V. Tzerpos, 
  <a href="https://doi.org/10.1109/SPED.2019.8906599">"FoR: A Dataset for Synthetic Speech Detection,"</a> 
  in International Conference on Speech Technology and Human-Computer Dialogue (SpeD), 2019, pp. 1–10.</p>

<p>[138] H. Nguyen, K. Li, and M. Unoki, 
  <a href="https://doi.org/10.21437/Interspeech.2022-528>"Automatic Mean Opinion Score Estimation with Temporal Modulation Features on Gammatone Filterbank 
  for Speech Assessment,"</a> in Proc. Interspeech, 2022, pp. 4526–4530.</p>

<p>[139]  Z. Yang, W. Zhou, C. Chu, S. Li, R. Dabre, R. Rubino, and Y. Zhao, 
  <a href="https://doi.org/10.21437/Interspeech.2022-10262">"Fusion of Self-supervised Learned Models for MOS Prediction,"</a> 
  in Proc. Interspeech, 2022, pp. 5443–5447.</p>

<p>[140] X. Tian, K. Fu, S. Gao, Y. Gu, K. Wang, W. Li, and Z. Ma, 
  <a href="https://doi.org/10.21437/Interspeech.2022-10022">"A Transfer and Multi-Task Learning based Approach for MOS Prediction,"</a> 
  in Proc. Interspeech, 2022, pp. 5438–5442.</p>

<p>[141] O. Plátek and O. Dusek, 
  <a href="https://doi.org/10.21437/SSW.2023-8>"MooseNet: A Trainable Metric for Synthesized Speech with a PLDA Module,"</a> 
  in Proc. 12th ISCA Speech Synthesis Workshop (SSW2023), 2023, pp. 48–54.</p>

<p>[142] A. Kunikoshi, J. Kim, W. Jun, and K. Sjölander, 
  <a href="https://arxiv.org/abs/2206.13817">"Comparison of Speech Representations for the MOS Prediction System,"</a> arXiv preprint arXiv:2206.13817, 2022.</p>

<p>[143] H. Becerra, A. Ragano, and A. Hines, 
  <a href="https://doi.org/10.21437/Interspeech.2022-10766">"Exploring the influence of fine-tuning data on wav2vec 2.0 model for blind speech quality prediction,"</a> in Proc. Interspeech 2022, 2022, pp. 4088–4092.</p>

<p>[144] M. Chinen, J. Skoglund, C. K. A. Reddy, A. Ragano, and A. Hines, 
  <a href="10.21437/Interspeech.2022-799">"Using Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset,"</a> 
  in Proc. Interspeech, 2022, pp. 4531–4535.</p>
    
  <h3>5. The VoiceMOS Challenge 2023</h3>


<p>[145] E. Cooper, W.-C. Huang, Y. Tsao, H.-M. Wang, T. Toda, and J. Yamagishi, 
  "The VoiceMOS Challenge 2023: Zero-Shot Subjective Speech Quality Prediction for Multiple Domains," 
  in Proc. ASRU, 2023.</p>
  
    <h4>5.1  Data and tracks</h4>


<p>[146] O. Perrotin, B. Stephenson, S. Gerber, and G. Bailly, "The Blizzard Challenge 2023," 
  in Proc. 18th Blizzard Challenge Workshop, 2023, pp. 1–27.</p>

<p>[147]  Y.-W. Chen and Y. Tsao, 
  "InQSS: a speech intelligibility and quality assessment model using a multitask learning network," 
  in Proc. Interspeech, 2022, pp. 3088–3092.</p>

<p>[148] R. E. Zezario, Y.-W. Chen, S.-W. Fu, Y. Tsao, H.-M. Wang, and C.-S. Fuh, 
  "A study on incorporating Whisper for robust speech assessment," 
  arXiv preprint arXiv:2309.12766, 2023.</p>

    <h4>5.2  Baselines</h4>

    <h4>5.3  Team approaches</h4>


<p>[149] Z. Qi, X. Hu, W. Zhou, S. Li, H.Wu, J. Lu, and X. Xu, 
  "LE-SSL-MOS: Self-Supervised Learning MOS Prediction with Listener Enhancement," 
  in Proc. ASRU, 2023.</p>

<p>[150]  K. Shen, D. Yan, L. Dong, Y. Ren, X. Wu, and J. Hu, 
  "SQAT-LD: SPeech Quality Assessment Transformer Utilizing Listener Dependent Modeling for 
  Zero-Shot Out-of-Domain MOS Prediction," in Proc. ASRU, 2023.</p>
    
  <h3>6.  Future prospects and challenges</h3>

<p>[151] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. Glass, 
  "SSAST: Self-Supervised Audio Spectrogram Transformer," 
  in Proc. AAAI, vol. 36, no. 10, 2022, pp. 10 699–10 709.
</p>

<p>[152] T. Saeki, S. Maiti, X. Li, S. Watanabe, S. Takamichi, and H. Saruwatari, 
  "Learning to speak from text: Zeroshot multilingual text-to-speech with unsupervised text pretraining," 
  in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence,
  IJCAI-23, E. Elkind, Ed. International Joint Conferences on Artificial Intelligence Organization,
  8 2023, pp. 5179–5187, main Track.</p>

<p>[153] J. Chevelu, D. Lolive, S. L. Maguer, and D. Guennec, 
  "How to compare TTS systems: a new subjective evaluation methodology focused on differences," 
  in Proc. Interspeech 2015, 2015, pp. 3481–3485.</p>







